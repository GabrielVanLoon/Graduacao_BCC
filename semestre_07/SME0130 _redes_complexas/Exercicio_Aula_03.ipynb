{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0d7597f",
   "metadata": {},
   "source": [
    "# Exercício Aula 03 - Distância e Correlações\n",
    "\n",
    "Author: Gabriel Van Loon\n",
    "Prof.:  Francisco Aparecido Rodrigues\n",
    "Universidade de São Paulo, São Carlos, Brasil.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaafe63",
   "metadata": {},
   "source": [
    "## Definindo as Funções e Bibliotecas utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95066816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c834babd",
   "metadata": {},
   "source": [
    "Funções vistas na aula do exercício 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "763acee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_path_avg_var(G):\n",
    "    dists = np.zeros(len(G)*(len(G)-1))\n",
    "    paths = np.zeros((len(G),len(G)))    \n",
    "    aux = 0\n",
    "    for i in range(len(G)):\n",
    "        print(\"Calculating row:\", i)\n",
    "        for j in range(i+1, len(G)):\n",
    "            if i != j:\n",
    "                dists[aux] = dists[(aux+1)] = len(nx.shortest_path(G,i,j))-1\n",
    "                paths[i,j] = paths[j,i] = dists[aux]\n",
    "                aux        += 2\n",
    "                \n",
    "    return np.average(dists), np.var(dists), paths\n",
    "\n",
    "def pearson_degree_correlation(G):\n",
    "    ki = []\n",
    "    kj = []\n",
    "    for i in range(0,len(G.nodes())):\n",
    "        for j in range(0, len(G.nodes())):\n",
    "            if(G.has_edge(i,j) == True):\n",
    "                ki.append(G.degree(i))\n",
    "                kj.append(G.degree(j))\n",
    "    \n",
    "    corr, _ = pearsonr(ki, kj)\n",
    "    return corr, ki, kj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44c6bc3",
   "metadata": {},
   "source": [
    "Funções vistas na aula do exercício 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68c6e704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_degree(G):\n",
    "    vk = np.array([v for v in dict(G.degree()).values()])\n",
    "    return vk, avg_degree\n",
    "\n",
    "# Optimized version by networkx documentation\n",
    "def degree_distribution(G, normalize=False):\n",
    "    degree_sequence = sorted([d for n, d in G.degree()], reverse=True)\n",
    "    degree_count    = collections.Counter(degree_sequence)\n",
    "    \n",
    "    if normalize:\n",
    "        total_sum = sum(degree_count.values())\n",
    "        for degree in degree_count:\n",
    "            degree_count[degree] = degree_count[degree]/total_sum\n",
    "            \n",
    "    return degree_count\n",
    "\n",
    "# Francisco originals\n",
    "def degree_distribution_original(G):\n",
    "    vk = dict(G.degree())\n",
    "    vk = list(vk.values())  # we get only the degree values\n",
    "    vk = np.array(vk)\n",
    "    maxk = np.max(vk)\n",
    "    mink = np.min(vk)\n",
    "    kvalues= np.arange(0,maxk+1) # possible values of k\n",
    "    Pk = np.zeros(maxk+1) # P(k)\n",
    "    for k in vk:\n",
    "        Pk[k] = Pk[k] + 1\n",
    "    Pk = Pk/sum(Pk) # the sum of the elements of P(k) must to be equal to one\n",
    "    return kvalues,Pk\n",
    "\n",
    "def momment_of_degree_distribution(G,m):\n",
    "    M = 0\n",
    "    N = len(G)\n",
    "    for i in G.nodes:\n",
    "        M = M + G.degree(i)**m\n",
    "    M = M/N\n",
    "    return M\n",
    "\n",
    "def network_variance(G):\n",
    "    variance = momment_of_degree_distribution(G,2) - momment_of_degree_distribution(G,1)**2\n",
    "    return variance\n",
    "\n",
    "def shannon_entropy(G):\n",
    "    k,Pk = degree_distribution_original(G)\n",
    "    H = 0\n",
    "    for p in Pk:\n",
    "        if(p > 0):\n",
    "            H = H - p*np.math.log(p, 2)\n",
    "    return H\n",
    "\n",
    "def normalized_shannon_entropy(G):\n",
    "    k,Pk = degree_distribution_original(G)\n",
    "    H = 0\n",
    "    for p in Pk:\n",
    "        if(p > 0):\n",
    "            H = H - p*np.math.log(p, 2)\n",
    "    return H/np.math.log(len(G),2)\n",
    "\n",
    "def network_complexity_coef(G):\n",
    "    return (momment_of_degree_distribution(G,2) / momment_of_degree_distribution(G,1))\n",
    "\n",
    "# Just to remember the calls\n",
    "# nx.transitivity(G)\n",
    "# nx.average_clustering(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae3f615",
   "metadata": {},
   "source": [
    "## 1) Para a rede “Hamsterster”, calcule a média dos menores caminhos e o diâmetro. Use apenas o maior componente da rede e remova ciclos ou auto-conexões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f492ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and prepare the network\n",
    "G = nx.read_edgelist('data/hamsterster.txt', nodetype=int)\n",
    "# Grafo não direcionado\n",
    "G = G.to_undirected()\n",
    "# Remover auto-loops\n",
    "G.remove_edges_from(nx.selfloop_edges(G))\n",
    "# Escolhe maior componente\n",
    "Gcc = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "G = G.subgraph(Gcc[0])\n",
    "# Renomeia os vértices\n",
    "G = nx.convert_node_labels_to_integers(G, first_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3881da1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average shortest path length: 3.4526\n",
      "Network diameter: 14\n"
     ]
    }
   ],
   "source": [
    "print(\"Average shortest path length:\", \"%3.4f\"%nx.average_shortest_path_length(G))\n",
    "print('Network diameter:', nx.diameter(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a5215c",
   "metadata": {},
   "source": [
    "## 2) Considere a rede “USairport500” e calculea média e variância dos menores caminhos. Use apenas o maior componente da rede e remova ciclos ou auto-conexões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59bdd1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and prepare the network\n",
    "G = nx.read_edgelist('data/USairport500.txt', nodetype=int)\n",
    "# Grafo não direcionado\n",
    "G = G.to_undirected()\n",
    "# Remover auto-loops\n",
    "G.remove_edges_from(nx.selfloop_edges(G))\n",
    "# Escolhe maior componente\n",
    "Gcc = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "G = G.subgraph(Gcc[0])\n",
    "# Renomeia os vértices\n",
    "G = nx.convert_node_labels_to_integers(G, first_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200f6308",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "avg, var, short_path_dist = short_path_avg_var(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "334f8465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg shortest path: 2.9910\n",
      "Variance shortest path: 0.8175\n"
     ]
    }
   ],
   "source": [
    "print(\"Avg shortest path:\", \"%3.4f\"%avg)\n",
    "print(\"Variance shortest path:\", \"%3.4f\"%var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dd359d",
   "metadata": {},
   "source": [
    "## 3) Para a rede “USairport500”, calcule a entropia de Shannon da distribuição dos menores caminhos. Use logaritmo na base 2 e considere apenas o maior componente da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ca1a7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shannon Entropy: 1.9007137451744505\n"
     ]
    }
   ],
   "source": [
    "# Criando a distribuição de distancias\n",
    "path_dist = np.zeros(nx.diameter(G)+1)\n",
    "\n",
    "for i in range(0,len(G.nodes())):\n",
    "    for j in range(0, len(G.nodes())):\n",
    "        dij = int(short_path_dist[i,j])\n",
    "        path_dist[dij] += 1.0\n",
    "        \n",
    "path_dist = path_dist/np.sum(path_dist)\n",
    "\n",
    "from scipy.stats import entropy\n",
    "print('Shannon Entropy:', entropy(path_dist, base=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a3373310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.00000e-03, 2.38400e-02, 2.80136e-01, 4.31960e-01, 2.09184e-01,\n",
       "       4.90560e-02, 3.72800e-03, 9.60000e-05])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5901922",
   "metadata": {},
   "source": [
    "## 4) Calcule o coeficiente de assortatividade da rede Advogato. Considere apenas o maior componente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe8124cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and prepare the network\n",
    "G = nx.read_edgelist('data/advogato.txt', nodetype=int, data=(('weight',float),) )\n",
    "# Grafo não direcionado\n",
    "G = G.to_undirected()\n",
    "# Remover auto-loops\n",
    "G.remove_edges_from(nx.selfloop_edges(G))\n",
    "# Escolhe maior componente\n",
    "Gcc = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "G = G.subgraph(Gcc[0])\n",
    "# Renomeia os vértices\n",
    "G = nx.convert_node_labels_to_integers(G, first_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44686b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assortativity =  -0.0957\n"
     ]
    }
   ],
   "source": [
    "print(\"Assortativity = \",\"%3.4f\"%nx.degree_assortativity_coefficient(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7d0cca",
   "metadata": {},
   "source": [
    "## 5) Calcule o coeficiente de correlação de Pearson entre o grau médio dos vizinhos e o grau de cada vértice para a rede “word_adjacencies”. Isto é, entre k e knn(k). Use apenas o maior componente.Considere o exemplo da aula. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ec766ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and prepare the network\n",
    "G = nx.read_edgelist('data/word_adjacencies.txt', nodetype=int, data=(('weight',float),) )\n",
    "# Grafo não direcionado\n",
    "G = G.to_undirected()\n",
    "# Remover auto-loops\n",
    "G.remove_edges_from(nx.selfloop_edges(G))\n",
    "# Escolhe maior componente\n",
    "Gcc = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "G = G.subgraph(Gcc[0])\n",
    "# Renomeia os vértices\n",
    "G = nx.convert_node_labels_to_integers(G, first_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c70f11e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearsons correlation: -0.12934785343900132\n"
     ]
    }
   ],
   "source": [
    "corr, ki, kj = pearson_degree_correlation(G)\n",
    "print('Pearsons correlation:', corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4bfe0876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average degree of the neighborhood of the network: 14.76\n"
     ]
    }
   ],
   "source": [
    "knn = []\n",
    "for i in G.nodes():\n",
    "    aux =  nx.average_neighbor_degree(G, nodes = [i])\n",
    "    knn.append(float(aux[i]))\n",
    "knn = np.array(knn)\n",
    "print(\"Average degree of the neighborhood of the network:\", \"%3.2f\"%np.mean(knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9bcc98ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: -0.710832214935246\n"
     ]
    }
   ],
   "source": [
    "vk = dict(G.degree())\n",
    "vk = list(vk.values())\n",
    "\n",
    "knnk = list()\n",
    "ks = list()\n",
    "for k in np.arange(np.min(vk), np.max(vk)+1):\n",
    "    aux = vk == k\n",
    "    if(len(knn[aux]) > 0):\n",
    "        av_knn = np.mean(knn[aux]) #average clustering among all the nodes with degree k\n",
    "        knnk.append(av_knn)\n",
    "        ks.append(k)\n",
    "        \n",
    "\n",
    "\n",
    "rho = np.corrcoef(ks, knnk)[0,1]\n",
    "print('Pearson correlation coefficient:', rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeef949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
