{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0d7597f",
   "metadata": {},
   "source": [
    "# Exercício Aula 04 - Centralidade\n",
    "\n",
    "Author: Gabriel Van Loon\n",
    "Prof.:  Francisco Aparecido Rodrigues\n",
    "Universidade de São Paulo, São Carlos, Brasil.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaafe63",
   "metadata": {},
   "source": [
    "## Definindo as Funções e Bibliotecas utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95066816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a943f3b8",
   "metadata": {},
   "source": [
    "Funções vistas na aula do exercício da aula 04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6b4096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todas opções de centralidades são implementadas pelo networkx (muito eficiente u.u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c834babd",
   "metadata": {},
   "source": [
    "Funções vistas na aula do exercício da aula 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "763acee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_path_avg_var(G):\n",
    "    dists = np.zeros(len(G)*(len(G)-1))\n",
    "    paths = np.zeros((len(G),len(G)))    \n",
    "    aux = 0\n",
    "    for i in range(len(G)):\n",
    "        print(\"Calculating row:\", i)\n",
    "        for j in range(i+1, len(G)):\n",
    "            if i != j:\n",
    "                dists[aux] = dists[(aux+1)] = len(nx.shortest_path(G,i,j))-1\n",
    "                paths[i,j] = paths[j,i] = dists[aux]\n",
    "                aux        += 2\n",
    "                \n",
    "    return np.average(dists), np.var(dists), paths\n",
    "\n",
    "def pearson_degree_correlation(G):\n",
    "    ki = []\n",
    "    kj = []\n",
    "    for i in range(0,len(G.nodes())):\n",
    "        for j in range(0, len(G.nodes())):\n",
    "            if(G.has_edge(i,j) == True):\n",
    "                ki.append(G.degree(i))\n",
    "                kj.append(G.degree(j))\n",
    "    \n",
    "    corr, _ = pearsonr(ki, kj)\n",
    "    return corr, ki, kj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44c6bc3",
   "metadata": {},
   "source": [
    "Funções vistas na aula do exercício da aula 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68c6e704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_degree(G):\n",
    "    vk = np.array([v for v in dict(G.degree()).values()])\n",
    "    return vk, avg_degree\n",
    "\n",
    "# Optimized version by networkx documentation\n",
    "def degree_distribution(G, normalize=False):\n",
    "    degree_sequence = sorted([d for n, d in G.degree()], reverse=True)\n",
    "    degree_count    = collections.Counter(degree_sequence)\n",
    "    \n",
    "    if normalize:\n",
    "        total_sum = sum(degree_count.values())\n",
    "        for degree in degree_count:\n",
    "            degree_count[degree] = degree_count[degree]/total_sum\n",
    "            \n",
    "    return degree_count\n",
    "\n",
    "# Francisco originals\n",
    "def degree_distribution_original(G):\n",
    "    vk = dict(G.degree())\n",
    "    vk = list(vk.values())  # we get only the degree values\n",
    "    vk = np.array(vk)\n",
    "    maxk = np.max(vk)\n",
    "    mink = np.min(vk)\n",
    "    kvalues= np.arange(0,maxk+1) # possible values of k\n",
    "    Pk = np.zeros(maxk+1) # P(k)\n",
    "    for k in vk:\n",
    "        Pk[k] = Pk[k] + 1\n",
    "    Pk = Pk/sum(Pk) # the sum of the elements of P(k) must to be equal to one\n",
    "    return kvalues,Pk\n",
    "\n",
    "def momment_of_degree_distribution(G,m):\n",
    "    M = 0\n",
    "    N = len(G)\n",
    "    for i in G.nodes:\n",
    "        M = M + G.degree(i)**m\n",
    "    M = M/N\n",
    "    return M\n",
    "\n",
    "def network_variance(G):\n",
    "    variance = momment_of_degree_distribution(G,2) - momment_of_degree_distribution(G,1)**2\n",
    "    return variance\n",
    "\n",
    "def shannon_entropy(G):\n",
    "    k,Pk = degree_distribution_original(G)\n",
    "    H = 0\n",
    "    for p in Pk:\n",
    "        if(p > 0):\n",
    "            H = H - p*np.math.log(p, 2)\n",
    "    return H\n",
    "\n",
    "def normalized_shannon_entropy(G):\n",
    "    k,Pk = degree_distribution_original(G)\n",
    "    H = 0\n",
    "    for p in Pk:\n",
    "        if(p > 0):\n",
    "            H = H - p*np.math.log(p, 2)\n",
    "    return H/np.math.log(len(G),2)\n",
    "\n",
    "def network_complexity_coef(G):\n",
    "    return (momment_of_degree_distribution(G,2) / momment_of_degree_distribution(G,1))\n",
    "\n",
    "# Just to remember the calls\n",
    "# nx.transitivity(G)\n",
    "# nx.average_clustering(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae3f615",
   "metadata": {},
   "source": [
    "## 1) A media da medida eigenvector centrality da rede USairport500 é igual a quanto? Considere apenas o maior componente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f492ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and prepare the network\n",
    "G = nx.read_edgelist('data/USairport500.txt', nodetype=int)\n",
    "# Grafo não direcionado\n",
    "G = G.to_undirected()\n",
    "# Remover auto-loops\n",
    "G.remove_edges_from(nx.selfloop_edges(G))\n",
    "# Escolhe maior componente\n",
    "Gcc = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "G = G.subgraph(Gcc[0])\n",
    "# Renomeia os vértices\n",
    "G = nx.convert_node_labels_to_integers(G, first_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a0e9d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average eigenvector centrality 0.022754398239423695\n"
     ]
    }
   ],
   "source": [
    "# Calculating the Eigenvector with numerical methods \n",
    "eigen_centrality    = dict(nx.eigenvector_centrality(G, max_iter = 1000))\n",
    "avg_eigen_centrality = np.mean(list(eigen_centrality.values()))\n",
    "\n",
    "print('Average eigenvector centrality', avg_eigen_centrality)\n",
    "# print('Eigenvetor centrality:', EC[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a5215c",
   "metadata": {},
   "source": [
    "## 2) Calcule a Correlação de Pearson entre as medidas Betweeness e Grau Centrality para a rede Hamsterster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59bdd1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and prepare the network\n",
    "G = nx.read_edgelist('data/hamsterster.txt', nodetype=int)\n",
    "# Grafo não direcionado\n",
    "G = G.to_undirected()\n",
    "# Remover auto-loops\n",
    "G.remove_edges_from(nx.selfloop_edges(G))\n",
    "# Escolhe maior componente\n",
    "Gcc = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "G = G.subgraph(Gcc[0])\n",
    "# Renomeia os vértices\n",
    "G = nx.convert_node_labels_to_integers(G, first_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "200f6308",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bet_centrality    = dict(nx.betweenness_centrality(G))\n",
    "degree_centrality = dict(nx.degree_centrality(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "334f8465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation: 0.8242\n"
     ]
    }
   ],
   "source": [
    "pearson_corr, _ = pearsonr(list(bet_centrality.values()), list(degree_centrality.values()))\n",
    "print(\"Pearson Correlation:\", \"%3.4f\"%pearson_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dd359d",
   "metadata": {},
   "source": [
    "## 3) Calcule o coeficiênte de relação de Spearman entre as medidas de Closeness Centrality e K-Core para a rede Jazz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93058073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and prepare the network\n",
    "G = nx.read_edgelist('data/jazz.txt', nodetype=int)\n",
    "# Grafo não direcionado\n",
    "G = G.to_undirected()\n",
    "# Remover auto-loops\n",
    "G.remove_edges_from(nx.selfloop_edges(G))\n",
    "# Escolhe maior componente\n",
    "Gcc = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "G = G.subgraph(Gcc[0])\n",
    "# Renomeia os vértices\n",
    "G = nx.convert_node_labels_to_integers(G, first_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76d047fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "close_centrality  = dict(nx.closeness_centrality(G))\n",
    "k_core = dict(nx.core_number(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ca1a7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation: 0.7353\n"
     ]
    }
   ],
   "source": [
    "spearmanr_corr, _ = spearmanr(list(close_centrality.values()), list(k_core.values()))\n",
    "print(\"Spearman Correlation:\", \"%3.4f\"%spearmanr_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5901922",
   "metadata": {},
   "source": [
    "## 4) Medidas de Centralidade\n",
    "\n",
    "- **Betweness Centrality**: Menores caminhos de modo a determinar a carga (em termos de quantidade de tráfego) em cada vértice\n",
    "- **Eigen Vector Centrality**: Calculada a partir do espectro da matriz de adjacências\n",
    "- **Grau**: É uma medida de centralidade local, pois alguns vértices apresentando valores altos dessa medida podem estar nas bordas da rede\n",
    "- **PageRang**: Um vértice é considerado central se estiver conectado a outros vértices centrais\n",
    "- **Random Walk Acessibility**: Calculado a partir da exponencial da matriz de probabilidade de transição\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8795aa55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
